{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba3-JEPA Training Notebook\n",
    "\n",
    "Train the **Mamba3-JEPA** world model — a State-Space Vision-Language architecture\n",
    "that replaces the Transformer components of [VL-JEPA](https://arxiv.org/abs/2512.10942)\n",
    "with **Mamba-3** SSM blocks from `kore-mamba`.\n",
    "\n",
    "This notebook covers:\n",
    "1. Architecture overview\n",
    "2. Building the model from config\n",
    "3. Two-stage training pipeline\n",
    "4. Evaluation (embedding, classification, decoding)\n",
    "5. Selective decoding for streaming\n",
    "6. Scaling to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────────┐     ┌─────────────┐\n",
    "│  X-Encoder   │     │   Predictor       │     │  Y-Encoder   │\n",
    "│  (ViT,frozen)│────▶│  (Mamba-3 SSM)    │     │  (Mamba-3)   │\n",
    "│  image → Sv  │     │  ⟨Sv,Xq⟩ → Ŝy    │     │  text → Sy   │\n",
    "└─────────────┘     └────────┬───────────┘     └──────┬──────┘\n",
    "                             │                        │\n",
    "                             ▼                        ▼\n",
    "                     ┌──────────────────────────────────┐\n",
    "                     │   InfoNCE Loss: align Ŝy ↔ Sy   │\n",
    "                     └──────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼ (inference only)\n",
    "                     ┌──────────────────┐\n",
    "                     │   Y-Decoder       │\n",
    "                     │  (Mamba-3 LM)     │\n",
    "                     │   Ŝy → text       │\n",
    "                     └──────────────────┘\n",
    "```\n",
    "\n",
    "| Component | Original VL-JEPA | Mamba3-JEPA (Ours) |\n",
    "|-----------|-----------------|--------------------|\n",
    "| **X-Encoder** | V-JEPA 2 ViT-L (304M, frozen) | Same (ViT, frozen) |\n",
    "| **Predictor** | Llama-3.2-1B (8 Transformer layers) | Mamba-3 MixerModel (12 SSM layers) |\n",
    "| **Y-Encoder** | EmbeddingGemma-300M | Mamba-3 MixerModel (text encoder) |\n",
    "| **Y-Decoder** | Lightweight Transformer LM | Mamba-3 LM (MambaLMHeadModel) |\n",
    "\n",
    "### Why Mamba-3?\n",
    "\n",
    "| Feature | Benefit for VL-JEPA |\n",
    "|---------|--------------------|\n",
    "| **Complex-valued A** (data-dependent RoPE) | Tracks rotational dynamics in visual latent space |\n",
    "| **Trapezoidal discretization** | 2nd-order ODE accuracy for continuous video |\n",
    "| **MIMO multi-head** | High-throughput processing of ViT patch embeddings |\n",
    "| **O(N) sequence, O(1) decode** | Linear training, constant-time generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Build the `kore-vljepa` crate and run the training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build Kore (if not already done)\n",
    "!git clone https://github.com/KidIkaros/KORE.git 2>/dev/null || true\n",
    "%cd KORE\n",
    "\n",
    "# Verify the kore-vljepa crate compiles\n",
    "!cargo check -p kore-vljepa 2>&1 | tail -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full training example (tiny config, ~7 seconds)\n",
    "!cargo run --example mamba3_jepa_train -p kore-vljepa --release 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "The `Mamba3JepaConfig` struct controls all four components.\n",
    "\n",
    "### Tiny Preset (for testing)\n",
    "\n",
    "```rust\n",
    "let config = Mamba3JepaConfig::tiny();\n",
    "// ViT:       d_model=32,  layers=2,  patch=4×4,  image=16×16\n",
    "// Predictor: d_model=64,  layers=2,  d_state=16, headdim=16\n",
    "// Y-Encoder: d_model=64,  layers=2,  vocab=256\n",
    "// Y-Decoder: d_model=64,  layers=2,  prefix_len=4\n",
    "// Shared embed dim: 32\n",
    "```\n",
    "\n",
    "### Small Preset (~150M trainable params)\n",
    "\n",
    "```rust\n",
    "let config = Mamba3JepaConfig::small();\n",
    "// ViT:       d_model=1024, layers=24, patch=14×14, image=224×224 (V-JEPA 2 ViT-L)\n",
    "// Predictor: d_model=1024, layers=12, d_state=128, headdim=64\n",
    "// Y-Encoder: d_model=768,  layers=12, vocab=32000\n",
    "// Y-Decoder: d_model=512,  layers=6,  prefix_len=8\n",
    "// Shared embed dim: 1536\n",
    "```\n",
    "\n",
    "### Building the Model\n",
    "\n",
    "```rust\n",
    "use kore_vljepa::{Mamba3Jepa, Mamba3JepaConfig};\n",
    "\n",
    "let config = Mamba3JepaConfig::small();\n",
    "let model = Mamba3Jepa::new(config);\n",
    "// model.x_encoder  — VisionEncoder (ViT, frozen)\n",
    "// model.predictor   — Mamba3Predictor\n",
    "// model.y_encoder   — Mamba3TextEncoder\n",
    "// model.y_decoder   — Mamba3Decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline\n",
    "\n",
    "VL-JEPA training has **two stages** (§3.2 of the paper):\n",
    "\n",
    "### Stage 1: Query-Free Captioning Pretrain\n",
    "\n",
    "The model learns to predict text embeddings from visual input alone.\n",
    "Query tokens = target tokens (self-supervised).\n",
    "\n",
    "```rust\n",
    "// Stage 1: query = target (self-supervised)\n",
    "let query_tokens = target_tokens.clone();\n",
    "\n",
    "let output = model.train_forward(\n",
    "    &images,         // (batch, C, H, W) flattened\n",
    "    &query_tokens,   // (batch * n_qry,) flattened\n",
    "    &target_tokens,  // (batch * n_tgt,) flattened\n",
    "    batch, h, w,\n",
    "    n_qry, n_tgt,\n",
    "    temperature,     // τ = 0.07\n",
    ");\n",
    "// output.loss      — InfoNCE loss (scalar)\n",
    "// output.predicted — (batch, embed_dim)\n",
    "// output.target    — (batch, embed_dim)\n",
    "```\n",
    "\n",
    "### Stage 2: Query-Conditioned SFT\n",
    "\n",
    "The model learns to predict target embeddings given a visual input AND a query prompt.\n",
    "This is the key VL-JEPA innovation: predicting continuous embeddings instead of tokens.\n",
    "\n",
    "```rust\n",
    "// Stage 2: query ≠ target (supervised)\n",
    "let output = model.train_forward(\n",
    "    &images, &query_tokens, &target_tokens,\n",
    "    batch, h, w, n_qry, n_tgt, temperature,\n",
    ");\n",
    "```\n",
    "\n",
    "### Forward Pass Internals\n",
    "\n",
    "```\n",
    "train_forward(images, query_tokens, target_tokens):\n",
    "  1. X-Encoder:  images → visual_tokens     (ViT, frozen)\n",
    "  2. Embed:      query_tokens → query_embeds (shared embedding table)\n",
    "  3. Predictor:  (visual_tokens, query_embeds) → predicted_embedding\n",
    "     └─ VisionProjection → QueryProjection → concat → Mamba-3 layers → pool → proj\n",
    "  4. Y-Encoder:  target_tokens → target_embedding\n",
    "     └─ TokenEmbed → Mamba-3 layers → pool → proj\n",
    "  5. InfoNCE:    loss = -log(exp(sim(Ŝy,Sy)/τ) / Σ exp(sim(Ŝy,Sj)/τ))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Hyperparameters\n",
    "\n",
    "From VL-JEPA paper §3.2, adapted for Mamba-3:\n",
    "\n",
    "| Parameter | Stage 1 (Pretrain) | Stage 2 (SFT) |\n",
    "|-----------|-------------------|----------------|\n",
    "| **Steps** | 50,000 | 30,000 |\n",
    "| **Batch size** | 2,048 | 512 |\n",
    "| **Base LR** | 1e-4 | 5e-5 |\n",
    "| **Y-Encoder LR** | ×0.05 | ×0.05 |\n",
    "| **X-Encoder** | Frozen | Frozen |\n",
    "| **Optimizer** | AdamW | AdamW |\n",
    "| **β** | (0.9, 0.95) | (0.9, 0.95) |\n",
    "| **Weight decay** | 0.05 | 0.05 |\n",
    "| **LR schedule** | Cosine + 2K warmup | Cosine + 2K warmup |\n",
    "| **Temperature τ** | 0.07 | 0.07 |\n",
    "\n",
    "### Cosine LR Schedule\n",
    "\n",
    "```rust\n",
    "fn cosine_lr(step: usize, warmup: usize, total: usize, base_lr: f32) -> f32 {\n",
    "    if step < warmup {\n",
    "        base_lr * (step as f32 / warmup as f32)\n",
    "    } else {\n",
    "        let progress = (step - warmup) as f32 / (total - warmup) as f32;\n",
    "        base_lr * 0.5 * (1.0 + (PI * progress).cos())\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Differential Learning Rates\n",
    "\n",
    "The Y-Encoder trains at **5% of the base LR** to prevent catastrophic forgetting\n",
    "of pretrained text representations. The X-Encoder (ViT) is completely frozen.\n",
    "\n",
    "```rust\n",
    "// Parameter groups for AdamW:\n",
    "// Group 1: Predictor params     → lr = base_lr\n",
    "// Group 2: Y-Encoder params     → lr = base_lr * 0.05\n",
    "// Group 3: Y-Decoder params     → lr = base_lr\n",
    "// Group 4: X-Encoder params     → lr = 0 (frozen)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Pipeline\n",
    "\n",
    "Training data consists of `(visual_frames, query_text, target_text)` triplets.\n",
    "\n",
    "### Recommended Datasets\n",
    "\n",
    "| Dataset | Type | Size | Use |\n",
    "|---------|------|------|-----|\n",
    "| **CC3M** | Image-caption | 3M pairs | Stage 1 pretrain |\n",
    "| **CC12M** | Image-caption | 12M pairs | Stage 1 pretrain |\n",
    "| **WebVid-10M** | Video-caption | 10M clips | Stage 1 pretrain (video) |\n",
    "| **VQAv2** | Image-QA | 1.1M QA pairs | Stage 2 SFT |\n",
    "| **TextVQA** | Image-QA (OCR) | 45K images | Stage 2 SFT |\n",
    "| **Kinetics-400** | Video classification | 300K clips | Evaluation |\n",
    "\n",
    "### Data Format\n",
    "\n",
    "```rust\n",
    "// Each training sample:\n",
    "struct TrainSample {\n",
    "    image: Vec<f32>,         // (C, H, W) normalized to [-1, 1]\n",
    "    query_tokens: Vec<usize>,  // tokenized query text\n",
    "    target_tokens: Vec<usize>, // tokenized target text\n",
    "}\n",
    "\n",
    "// Stage 1: query_tokens == target_tokens (caption)\n",
    "// Stage 2: query_tokens = \"What is happening?\" , target_tokens = \"A cat is sleeping\"\n",
    "```\n",
    "\n",
    "### Image Preprocessing\n",
    "\n",
    "```rust\n",
    "// 1. Resize to (image_size × image_size) — e.g., 224×224\n",
    "// 2. Normalize: pixel = (pixel / 255.0 - mean) / std\n",
    "//    mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]\n",
    "// 3. Flatten to (C, H, W) = (3, 224, 224)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "After training, the model supports three inference modes:\n",
    "\n",
    "### 7a. Embedding Inference (Retrieval / Classification)\n",
    "\n",
    "```rust\n",
    "use kore_vljepa::{InferenceMode, InferenceOutput};\n",
    "\n",
    "let output = model.infer(&image, &query, h, w, InferenceMode::Embedding);\n",
    "if let InferenceOutput::Embedding(emb) = output {\n",
    "    // emb: Vec<f32> of shape (shared_embed_dim,)\n",
    "    // Use for nearest-neighbor classification or similarity ranking\n",
    "}\n",
    "```\n",
    "\n",
    "### 7b. Zero-Shot Classification\n",
    "\n",
    "Compare predicted embedding against candidate text embeddings:\n",
    "\n",
    "```rust\n",
    "// Encode class names through Y-Encoder\n",
    "let cat_emb = model.y_encoder.forward(&tokenize(\"cat\"), 1, n_tokens);\n",
    "let dog_emb = model.y_encoder.forward(&tokenize(\"dog\"), 1, n_tokens);\n",
    "let candidates = [cat_emb, dog_emb].concat();\n",
    "\n",
    "let class_idx = model.classify(&predicted_emb, &candidates, 2);\n",
    "// class_idx: 0 = cat, 1 = dog\n",
    "```\n",
    "\n",
    "### 7c. Text Decoding (VQA)\n",
    "\n",
    "```rust\n",
    "let output = model.infer(\n",
    "    &image, &query, h, w,\n",
    "    InferenceMode::Decode { max_tokens: 50, bos_token: 1 },\n",
    ");\n",
    "if let InferenceOutput::Tokens(tokens) = output {\n",
    "    let text = tokenizer.decode(&tokens);\n",
    "    println!(\"Answer: {}\", text);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Selective Decoding (Streaming)\n",
    "\n",
    "Mamba-3's hidden state `h_t` naturally tracks semantic change. The `SelectiveDecoder`\n",
    "monitors `‖h_t - h_{t-k}‖` and only triggers the expensive Y-Decoder when the state\n",
    "drifts past a threshold. Ideal for streaming video — decode only at scene changes.\n",
    "\n",
    "```rust\n",
    "use kore_vljepa::{SelectiveDecoder, SelectiveDecodeConfig};\n",
    "\n",
    "let config = SelectiveDecodeConfig {\n",
    "    window_size: 8,       // compare against state from 8 frames ago\n",
    "    drift_threshold: 0.5, // decode when drift exceeds this\n",
    "    smoothing: 0.3,       // exponential smoothing factor\n",
    "};\n",
    "let mut selective = SelectiveDecoder::new(config);\n",
    "\n",
    "// For each video frame:\n",
    "for frame in video_stream {\n",
    "    let ssm_state = get_predictor_state(&model, &frame);\n",
    "    if selective.should_decode(&ssm_state) {\n",
    "        let text = decode(&model, &frame, &query);\n",
    "        println!(\"Scene change detected: {}\", text);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This avoids running the decoder on every frame, reducing compute by 5-10× on\n",
    "typical video streams where most frames are visually similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Loading Pretrained Weights\n",
    "\n",
    "### V-JEPA 2 ViT Weights (X-Encoder)\n",
    "\n",
    "```rust\n",
    "use kore_vljepa::{load_safetensors, load_vit_weights};\n",
    "\n",
    "// Load V-JEPA 2 ViT-L weights from safetensors file\n",
    "let tensors = load_safetensors(\"vjepa2_vit_l.safetensors\")?;\n",
    "load_vit_weights(&mut model.x_encoder, &tensors)?;\n",
    "// X-Encoder is now frozen with pretrained weights\n",
    "```\n",
    "\n",
    "### Checkpoint Save/Load\n",
    "\n",
    "```rust\n",
    "// Save training checkpoint (predictor + y_encoder + y_decoder)\n",
    "// Use safetensors format for interoperability\n",
    "use safetensors::serialize;\n",
    "\n",
    "let state = collect_state_dict(&model);\n",
    "let bytes = serialize(&state, &None)?;\n",
    "std::fs::write(\"mamba3_jepa_checkpoint.safetensors\", bytes)?;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Scaling to Production\n",
    "\n",
    "### Step-by-Step\n",
    "\n",
    "1. **Replace synthetic data** with real image-text pairs (CC3M, WebVid)\n",
    "2. **Load V-JEPA 2 ViT weights** via `load_vit_weights()`\n",
    "3. **Use AdamW** from `kore-optim` with differential LR groups\n",
    "4. **Scale config** to `Mamba3JepaConfig::small()` (150M trainable params)\n",
    "5. **Add autograd** via `kore-autograd` for automatic differentiation\n",
    "6. **Multi-GPU** via data parallelism (split batches across devices)\n",
    "\n",
    "### Full Training Script Template\n",
    "\n",
    "```rust\n",
    "use kore_vljepa::*;\n",
    "\n",
    "fn main() {\n",
    "    // 1. Config\n",
    "    let config = Mamba3JepaConfig::small();\n",
    "    let model = Mamba3Jepa::new(config.clone());\n",
    "\n",
    "    // 2. Load pretrained ViT\n",
    "    let tensors = load_safetensors(\"vjepa2_vit_l.safetensors\").unwrap();\n",
    "    load_vit_weights(&mut model.x_encoder, &tensors).unwrap();\n",
    "\n",
    "    // 3. Optimizer (differential LR)\n",
    "    // let optimizer = AdamW::new([\n",
    "    //     ParamGroup::new(model.predictor.params(), base_lr),\n",
    "    //     ParamGroup::new(model.y_encoder.params(), base_lr * 0.05),\n",
    "    //     ParamGroup::new(model.y_decoder.params(), base_lr),\n",
    "    // ], weight_decay=0.05, betas=(0.9, 0.95));\n",
    "\n",
    "    // 4. Stage 1: Query-free pretrain\n",
    "    for step in 0..50_000 {\n",
    "        let (images, _, targets) = load_batch(\"cc3m\", batch=2048);\n",
    "        let queries = targets.clone(); // self-supervised\n",
    "        let lr = cosine_lr(step, 2000, 50_000, 1e-4);\n",
    "\n",
    "        let output = model.train_forward(\n",
    "            &images, &queries, &targets,\n",
    "            2048, 224, 224, n_qry, n_tgt, 0.07,\n",
    "        );\n",
    "        // optimizer.step(output.loss, lr);\n",
    "    }\n",
    "\n",
    "    // 5. Stage 2: Query-conditioned SFT\n",
    "    for step in 0..30_000 {\n",
    "        let (images, queries, targets) = load_batch(\"vqav2\", batch=512);\n",
    "        let lr = cosine_lr(step, 2000, 30_000, 5e-5);\n",
    "\n",
    "        let output = model.train_forward(\n",
    "            &images, &queries, &targets,\n",
    "            512, 224, 224, n_qry, n_tgt, 0.07,\n",
    "        );\n",
    "        // optimizer.step(output.loss, lr);\n",
    "    }\n",
    "\n",
    "    // 6. Save checkpoint\n",
    "    // save_checkpoint(&model, \"mamba3_jepa_small.safetensors\");\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "### Core Types\n",
    "\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| `Mamba3Jepa` | Full model: x_encoder + predictor + y_encoder + y_decoder |\n",
    "| `Mamba3JepaConfig` | Top-level config (presets: `tiny()`, `small()`) |\n",
    "| `TrainOutput` | Training forward result: `{ loss, predicted, target }` |\n",
    "| `InferenceMode` | `Embedding` or `Decode { max_tokens, bos_token }` |\n",
    "| `InferenceOutput` | `Embedding(Vec<f32>)` or `Tokens(Vec<usize>)` |\n",
    "| `SelectiveDecoder` | SSM state drift monitor for streaming decode |\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "| Method | Signature |\n",
    "|--------|-----------|\n",
    "| `Mamba3Jepa::new` | `(config: Mamba3JepaConfig) -> Self` |\n",
    "| `train_forward` | `(&self, images, query_tokens, target_tokens, batch, h, w, n_qry, n_tgt, temperature) -> TrainOutput` |\n",
    "| `infer` | `(&self, image, query_tokens, h, w, mode) -> InferenceOutput` |\n",
    "| `classify` | `(&self, predicted, candidates, n_candidates) -> usize` |\n",
    "| `info_nce_loss` | `(predictions, targets, batch, embed_dim, temperature) -> f32` |\n",
    "| `load_vit_weights` | `(&mut VisionEncoder, &SafeTensors) -> Result<()>` |\n",
    "\n",
    "### Config Structs\n",
    "\n",
    "| Config | Key Fields |\n",
    "|--------|------------|\n",
    "| `VitConfig` | `patch_size, image_size, d_model, n_heads, n_layers, d_ff` |\n",
    "| `Mamba3PredictorConfig` | `d_model, n_layers, d_state, expand, headdim, trapezoidal_alpha, embed_dim` |\n",
    "| `Mamba3TextEncoderConfig` | `vocab_size, d_model, n_layers, d_state, embed_dim` |\n",
    "| `Mamba3DecoderConfig` | `d_model, n_layers, vocab_size, prefix_len, embed_dim` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests to verify everything works\n",
    "!cargo test -p kore-vljepa 2>&1 | tail -15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
