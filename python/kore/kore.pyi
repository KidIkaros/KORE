"""Type stubs for the kore native extension (PyO3/maturin)."""

from __future__ import annotations
from typing import Dict, List, Optional, Sequence, overload
import numpy as np
import numpy.typing as npt

# ============================================================================
# Tensor
# ============================================================================

class Tensor:
    """A multi-dimensional array â€” the fundamental data structure in Kore."""

    def __init__(self, data: npt.NDArray[np.float32]) -> None: ...

    @staticmethod
    def zeros(shape: List[int]) -> Tensor: ...

    @staticmethod
    def ones(shape: List[int]) -> Tensor: ...

    @staticmethod
    def randn(shape: List[int]) -> Tensor: ...

    @staticmethod
    def rand_uniform(shape: List[int], low: float = 0.0, high: float = 1.0) -> Tensor: ...

    @staticmethod
    def cat(tensors: List[Tensor], axis: int) -> Tensor: ...

    @staticmethod
    def stack(tensors: List[Tensor], axis: int) -> Tensor: ...

    @property
    def shape(self) -> List[int]: ...

    @property
    def ndim(self) -> int: ...

    @property
    def numel(self) -> int: ...

    @property
    def dtype(self) -> str: ...

    @property
    def requires_grad(self) -> bool: ...

    @property
    def grad(self) -> Optional[Tensor]: ...

    def numpy(self) -> npt.NDArray[np.float32]: ...
    def add(self, other: Tensor) -> Tensor: ...
    def sub(self, other: Tensor) -> Tensor: ...
    def mul(self, other: Tensor) -> Tensor: ...
    def matmul(self, other: Tensor) -> Tensor: ...
    def reshape(self, shape: List[int]) -> Tensor: ...
    def transpose(self) -> Tensor: ...
    def sum(self) -> Tensor: ...
    def mean(self) -> Tensor: ...
    def abs(self) -> Tensor: ...
    def softmax(self, axis: int = -1) -> Tensor: ...
    def log_softmax(self, axis: int = -1) -> Tensor: ...
    def masked_fill(self, mask: Tensor, value: float) -> Tensor: ...
    def triu(self, k: int = 0) -> Tensor: ...
    def tril(self, k: int = 0) -> Tensor: ...
    def gather(self, axis: int, index: Tensor) -> Tensor: ...
    def requires_grad_(self, requires_grad: bool) -> None: ...
    def zero_grad(self) -> None: ...
    def backward(self) -> None: ...

    def __add__(self, other: Tensor) -> Tensor: ...
    def __sub__(self, other: Tensor) -> Tensor: ...
    def __mul__(self, other: Tensor) -> Tensor: ...
    def __matmul__(self, other: Tensor) -> Tensor: ...
    def __repr__(self) -> str: ...
    def __str__(self) -> str: ...

# ============================================================================
# nn
# ============================================================================

class nn:
    class Linear:
        def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def parameters(self) -> List[Tensor]: ...

    class LayerNorm:
        def __init__(self, normalized_shape: int, eps: float = 1e-5) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def parameters(self) -> List[Tensor]: ...

    class Embedding:
        def __init__(self, num_embeddings: int, embedding_dim: int) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def lookup(self, ids: List[int]) -> Tensor: ...
        def parameters(self) -> List[Tensor]: ...

    class BitLinear:
        def __init__(self, in_features: int, out_features: int, bias: bool = False, threshold: float = 0.3) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def compression_ratio(self) -> float: ...
        def __repr__(self) -> str: ...

    class QuatLinear:
        def __init__(self, in_features: int, out_features: int, bias: bool = False) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def compression_ratio(self) -> float: ...
        def __repr__(self) -> str: ...

    class LoraLinear:
        def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 8.0, bias: bool = False) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def trainable_params(self) -> int: ...
        def total_params(self) -> int: ...
        def parameters(self) -> List[Tensor]: ...
        def __repr__(self) -> str: ...

    class Dropout:
        def __init__(self, p: float = 0.5) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def train(self, mode: bool) -> None: ...
        def eval(self) -> None: ...

    class Conv2d:
        def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = True) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def parameters(self) -> List[Tensor]: ...

    class MaxPool2d:
        def __init__(self, kernel_size: int, stride: int = 2, padding: int = 0) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...

    class AvgPool2d:
        def __init__(self, kernel_size: int, stride: int = 2, padding: int = 0) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...

    class AdaptiveAvgPool2d:
        def __init__(self, output_h: int, output_w: int) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...

    class Sequential:
        def __init__(self, layers: Optional[List] = None) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __call__(self, input: Tensor) -> Tensor: ...
        def parameters(self) -> List[Tensor]: ...
        def train(self, mode: bool) -> None: ...
        def eval(self) -> None: ...
        def __len__(self) -> int: ...

# ============================================================================
# data
# ============================================================================

class data:
    class TensorDataset:
        def __init__(self, inputs: Tensor, targets: Tensor) -> None: ...
        def __len__(self) -> int: ...

    class DataLoader:
        def __init__(self, dataset: 'data.TensorDataset', batch_size: int = 32, shuffle: bool = False, drop_last: bool = False, seed: Optional[int] = None) -> None: ...
        def __len__(self) -> int: ...

# ============================================================================
# training
# ============================================================================

class training:
    class Trainer:
        def __init__(self, model: 'nn.Sequential', optimizer: object, loss: str = "mse", log_every: int = 1, grad_clip_norm: float = 0.0) -> None: ...
        def fit(self, loader: 'data.DataLoader', epochs: int) -> List[float]: ...
        def evaluate(self, loader: 'data.DataLoader') -> float: ...
        def predict(self, loader: 'data.DataLoader') -> List[Tensor]: ...
        @property
        def lr(self) -> float: ...
        @lr.setter
        def lr(self, value: float) -> None: ...

# ============================================================================
# optim
# ============================================================================

class optim:
    class Adam:
        def __init__(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8, weight_decay: float = 0.0) -> None: ...
        def step(self, params: List[Tensor], grads: List[Tensor]) -> None: ...

    class SGD:
        def __init__(self, lr: float = 0.01, momentum: float = 0.0, weight_decay: float = 0.0) -> None: ...
        def step(self, params: List[Tensor], grads: List[Tensor]) -> None: ...

# ============================================================================
# functional
# ============================================================================

class functional:
    @staticmethod
    def relu(input: Tensor) -> Tensor: ...
    @staticmethod
    def gelu(input: Tensor) -> Tensor: ...
    @staticmethod
    def softmax(input: Tensor) -> Tensor: ...
    @staticmethod
    def sigmoid(input: Tensor) -> Tensor: ...
    @staticmethod
    def tanh(input: Tensor) -> Tensor: ...
    @staticmethod
    def silu(input: Tensor) -> Tensor: ...
    @staticmethod
    def cross_entropy_loss(logits: Tensor, targets: Tensor) -> Tensor: ...
    @staticmethod
    def mse_loss(pred: Tensor, target: Tensor) -> Tensor: ...
    @staticmethod
    def l1_loss(pred: Tensor, target: Tensor) -> Tensor: ...
    @staticmethod
    def nll_loss(log_probs: Tensor, targets: Tensor) -> Tensor: ...

# ============================================================================
# Top-level functions
# ============================================================================

def relu(input: Tensor) -> Tensor: ...
def gelu(input: Tensor) -> Tensor: ...
def softmax(input: Tensor) -> Tensor: ...
def sigmoid(input: Tensor) -> Tensor: ...
def save_state_dict(state_dict: Dict[str, Tensor], path: str) -> None: ...
def load_state_dict(path: str) -> Dict[str, Tensor]: ...
